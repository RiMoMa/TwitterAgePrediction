{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "len8Qi91y5FX",
    "outputId": "5be3d953-afa4-4cea-d32e-8931f9613679",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: unidecode in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (1.3.4)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/home/ricardo/anaconda3/envs/whalejaguar/bin/python -m pip install --upgrade pip' command.\u001B[0m\n",
      "Requirement already satisfied: transformers in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (4.17.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: sacremoses in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: six in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/home/ricardo/anaconda3/envs/whalejaguar/bin/python -m pip install --upgrade pip' command.\u001B[0m\n",
      "Requirement already satisfied: spacy in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: setuptools in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (62.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (3.10.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy) (4.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/home/ricardo/anaconda3/envs/whalejaguar/bin/python -m pip install --upgrade pip' command.\u001B[0m\n",
      "Collecting es-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 12.9 MB 5.1 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: es_core_news_sm in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from es-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.21.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: setuptools in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (62.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/home/ricardo/anaconda3/envs/whalejaguar/bin/python -m pip install --upgrade pip' command.\u001B[0m\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install wget unidecode\n",
    "!pip install transformers\n",
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download es_core_news_sm 'es_core_news_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W6jl8x_KF8GK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#some libraries from https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
    "\n",
    "#library that contains punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "\n",
    "    punctuationfree=\"\".join([i if i not in string.punctuation else ' ' for i in text])\n",
    "    return punctuationfree\n",
    "\n",
    "def count_punctuation(text):\n",
    "    from numpy import sum\n",
    "    count = 0\n",
    "    punctuationfree=[len(i)+count for i in text if i in string.punctuation]\n",
    "    return sum(punctuationfree)\n",
    "\n",
    "\n",
    "def remove_hashs(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.sub(\"(#[A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return raw_tweet\n",
    "\n",
    "def separate_users(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.findall(\"(@[A-Za-z0-9_]+)\" ,text))\n",
    "    return raw_tweet\n",
    "\n",
    "\n",
    "def separate_hashtags(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.findall(\"(#[A-Za-z0-9]+)\", text))\n",
    "    return raw_tweet\n",
    "def separate_urls(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.findall(\"(https?://\\S+|www\\.\\S+)\", text))\n",
    "    return raw_tweet\n",
    "r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "\n",
    "def remove_users(text) :\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\",\" \",text).split())\n",
    "    return raw_tweet\n",
    "    #data['msg_lower'] = data['clean_msg'].apply(lambda x: x.lower())\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.sub(\"(#[A-Za-z0-9]+)\", \" \", text).split())\n",
    "    return raw_tweet\n",
    "    # data['msg_lower'] = data['clean_msg'].apply(lambda x: x.lower())\n",
    "\n",
    "def remove_hash_symbol(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.sub(\"(#)\", \"\", text).split())\n",
    "    return raw_tweet\n",
    "\n",
    "def remove_at_symbol(text):\n",
    "    import re\n",
    "    raw_tweet = ' '.join(re.sub(\"(@)\", \"\", text).split())\n",
    "    return raw_tweet\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "def remove_stopwords(text):\n",
    "    import nltk\n",
    "   # nltk.download('stopwords')\n",
    "    stopword = nltk.corpus.stopwords.words('spanish')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopword])\n",
    "    return text\n",
    "\n",
    "def Separate_hashtag_words(s):\n",
    "    import re\n",
    "    return ' '.join(re.split(r'(?=[A-Z])', s))\n",
    "\n",
    "def Separate_at_words(s):\n",
    "    import re\n",
    "    return ' '.join(re.split(r'(?=[A-Z])', s))\n",
    "\n",
    "\n",
    "\n",
    "#def separate_hash(text):\n",
    "  #  text = ' '.join([word for word in text.split() if word not in stopword])\n",
    "\n",
    "\n",
    "def lemmatization(text):\n",
    "  #ToDo: try other lemma Stanford CoreNLP or FreeLing. \n",
    "  \n",
    "    import spacy\n",
    "    import es_core_news_sm\n",
    "    nlp = es_core_news_sm.load()\n",
    "   # nlp = spacy.load(\"es_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    Lemma = doc[:].lemma_\n",
    "    return Lemma\n",
    "\n",
    "\n",
    "def Build_dictionary(data):\n",
    "    import pandas as pd\n",
    "    data = pd.DataFrame(data)\n",
    "    import unidecode\n",
    "\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "#    dataC = ' '.join(data.to_list())\n",
    "    data['copy'] = data['text']\n",
    "    data['text'] = data['text'].apply(lambda x: remove_hashs(x))\n",
    "    data['text'] = data.text.apply(lambda x: unidecode.unidecode(x))\n",
    "    data['text'] = data['text'].apply(lambda x: remove_users(x))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    data['text'] = data.text.apply(lambda x: stemmer.stem(x))  # to remove s (to singular)\n",
    "#    data['text4'] = data.text.apply(lambda x: remove_stopwords(x))  # remove stop words, english and spanish\n",
    "    data[\"text\"] = data[\"text\"].str.replace(r'https?://\\S+|www\\.\\S+', ' ').str.strip()  # to delete all URLS\n",
    "    data[\"text\"] = data[\"text\"].str.replace(r'\\n', ' ').str.strip()  # to delete all '\\n'\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    # data['text'] = data.text.apply(lambda x: re.split('@\\/\\/*', str(x))[0]) # to delete all URLS\n",
    "    all_words = ' '.join(data.text.to_list()).split()\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=1000,oov_token=\"<OOV>\",char_level=True)\n",
    "    tokenizer.fit_on_texts(all_words)\n",
    "    word_index = tokenizer.word_index\n",
    "    Tokens_words = tokenizer.texts_to_sequences(all_words)\n",
    "    maxlen = 0\n",
    "    for i in range(len(Tokens_words)):\n",
    "        if len(Tokens_words[i]) > maxlen:\n",
    "            maxlen = len(Tokens_words[i])\n",
    "            idx_large = i\n",
    "            print(all_words[idx_large])\n",
    "\n",
    "\n",
    "    Tokens_padded = tf.keras.preprocessing.sequence.pad_sequences(Tokens_words, maxlen=19, padding='post')\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "    import numpy as np\n",
    "\n",
    "    idx_ = np.random.permutation(len(Tokens_padded))\n",
    "    idx_sel = idx_[0:200000]\n",
    "\n",
    "    Tokens_words = tokenizer.texts_to_matrix(all_words, mode='freq')\n",
    "    tokens_sel = Tokens_words[idx_sel]\n",
    "    from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "    n_samples = 50000\n",
    "    n_features = 50000\n",
    "    n_components = 250\n",
    "    n_top_words = 20\n",
    "    nmf = NMF(n_components=n_components, random_state=1, alpha=0.1, l1_ratio=0.5).fit(tokens_sel)\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_components,\n",
    "        max_iter=5,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(tokens_sel)\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=400, random_state=0).fit(tokens_sel)\n",
    "    kmedoids.labels_\n",
    "\n",
    "    kmedoids.predict([[0,0], [4,4]])\n",
    "\n",
    "    kmedoids.cluster_centers_\n",
    "\n",
    "\n",
    "    kmedoids.inertia_\n",
    "\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(5, 10, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 10})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=7)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KFwMBAQs4yum",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "###OPTIONS\n",
    "data = pd.read_json('/home/ricardo/WhaleAndJaguar/test_ds_ml-main/data/clasificador/clasificador.json') #To open json of the raw data\n",
    "\n",
    "cleanDataPath = '/home/ricardo/WhaleAndJaguar/test_ds_ml-main//DataClean.pkl' # Path to save the postprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...</td>\n",
       "      <td>50-64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jlmorales5 Sol Campbell, Per Mertesacker, Ver...</td>\n",
       "      <td>13-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LA DIGNIDAD COMIENZA POR RESPETAR LA VIDA Y NO...</td>\n",
       "      <td>18-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@munqueik roba3</td>\n",
       "      <td>18-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Necesito ayuda</td>\n",
       "      <td>18-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text age_range\n",
       "0  @gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...     50-64\n",
       "1  @jlmorales5 Sol Campbell, Per Mertesacker, Ver...     13-17\n",
       "2  LA DIGNIDAD COMIENZA POR RESPETAR LA VIDA Y NO...     18-24\n",
       "3                                    @munqueik roba3     18-24\n",
       "4                                     Necesito ayuda     18-24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kTcFDow7Kcm",
    "outputId": "bc24509f-4d90-44a5-bfb6-40f58c1d23d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kTcFDow7Kcm",
    "outputId": "bc24509f-4d90-44a5-bfb6-40f58c1d23d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOjzwIc307GV",
    "outputId": "f5b589e1-154f-47c1-d9c9-fac9c85524c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "/home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import  Pool\n",
    "import  numpy as np\n",
    "import re\n",
    "import os.path\n",
    "#import functions to clean data\n",
    "import unidecode\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#load Original Data\n",
    "label_text = data['age_range'].unique()#To determine ranges or classes\n",
    "label_text.sort() # organize the age ranges\n",
    "for n in range(len(label_text)):\n",
    "    data['age_range']=data.age_range.replace(label_text[n], n) #to remplace the str class with a numerical class\n",
    "\n",
    "label_n = np.array(range(len(label_text)))#To define numerical classes associated with age-ranges\n",
    "# to divide the data in user, hasgtags and text\n",
    "data['users']=data.text.apply(lambda x:separate_users(x))\n",
    "data['Hashtags']=data.text.apply(lambda x:separate_hashtags(x))\n",
    "data['copy']=data['text']\n",
    "stemmer = PorterStemmer() #stemmer class\n",
    "data['text'] = data.text.apply(lambda x: stemmer.stem(x)) #to remove s (to singular)\n",
    "data['text'] = data.text.apply(lambda x: unidecode.unidecode(x)) #to covert all in ascii\n",
    "data['text'] = data.text.apply(lambda x: remove_users(x)) # to remove users\n",
    "data['text'] = data.text.apply(lambda x: remove_hashtags(x)) #to remove hastags\n",
    "data['text'] = data.text.apply(lambda x: remove_stopwords(x)) # remove stop words, and spanish\n",
    "data[\"text\"] = data[\"text\"].str.replace(r'https?://\\S+|www\\.\\S+', ' ').str.strip() # to delete all URLS\n",
    "data[\"text\"] = data[\"text\"].str.replace(r'\\n', ' ').str.strip() # to delete all '\\n'\n",
    "data['text'] = data['text'].apply(lambda x:remove_punctuation(x)) #delete punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                          amen\n",
       "age_range                        4\n",
       "users             @oracion_milagro\n",
       "Hashtags                          \n",
       "copy         @oracion_milagro Amén\n",
       "Name: 42, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOjzwIc307GV",
    "outputId": "f5b589e1-154f-47c1-d9c9-fac9c85524c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open lemma\n",
      "Time:  5.434652794996509\n"
     ]
    }
   ],
   "source": [
    "### FUNCTION TO LEMMAZATION THE DATA IN PARALLEL\n",
    "def lemmaParallel(data):\n",
    "    data['textLemma'] = data['text'].apply(lambda x:lemmatization(x))\n",
    "    return data\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=16):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "### IF DOES NOT EXIST THE LEMMA DATA THEN EXCUTE THE FUNCTION IF EXIST OPEN THE DATA\n",
    "if not os.path.exists(cleanDataPath):\n",
    "    print('Lemma')\n",
    "    data = parallelize_dataframe(data,lemmaParallel)\n",
    "    data.to_pickle(cleanDataPath)\n",
    "    \n",
    "else:\n",
    "    print('open lemma')\n",
    "    data = pd.read_pickle(cleanDataPath)\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                          amen\n",
       "age_range                        4\n",
       "users             @oracion_milagro\n",
       "Hashtags                          \n",
       "copy         @oracion_milagro Amén\n",
       "textLemma                     amen\n",
       "Name: 42, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age_range</th>\n",
       "      <th>users</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>copy</th>\n",
       "      <th>textLemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelo huawei don gustavo</td>\n",
       "      <td>4</td>\n",
       "      <td>@gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...</td>\n",
       "      <td></td>\n",
       "      <td>@gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...</td>\n",
       "      <td>modelo huawei don gustavo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sol campbell  per mertesacker  vermaelen chapa...</td>\n",
       "      <td>0</td>\n",
       "      <td>@jlmorales5</td>\n",
       "      <td></td>\n",
       "      <td>@jlmorales5 Sol Campbell, Per Mertesacker, Ver...</td>\n",
       "      <td>sol campbell  per mertesacker  vermaelar chapa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dignidad comienza respetar vida asesinato sele...</td>\n",
       "      <td>1</td>\n",
       "      <td>@La</td>\n",
       "      <td></td>\n",
       "      <td>LA DIGNIDAD COMIENZA POR RESPETAR LA VIDA Y NO...</td>\n",
       "      <td>dignidad comenzar respetar vida asesinato sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roba3</td>\n",
       "      <td>1</td>\n",
       "      <td>@munqueik</td>\n",
       "      <td></td>\n",
       "      <td>@munqueik roba3</td>\n",
       "      <td>roba3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>necesito ayuda</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Necesito ayuda</td>\n",
       "      <td>necesitar ayuda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  age_range  \\\n",
       "0                         modelo huawei don gustavo           4   \n",
       "1  sol campbell  per mertesacker  vermaelen chapa...          0   \n",
       "2  dignidad comienza respetar vida asesinato sele...          1   \n",
       "3                                              roba3          1   \n",
       "4                                     necesito ayuda          1   \n",
       "\n",
       "                                               users Hashtags  \\\n",
       "0  @gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...            \n",
       "1                                        @jlmorales5            \n",
       "2                                                @La            \n",
       "3                                          @munqueik            \n",
       "4                                                               \n",
       "\n",
       "                                                copy  \\\n",
       "0  @gusgomez1701 @YaKoDj @TropiBogota @Apple @Hua...   \n",
       "1  @jlmorales5 Sol Campbell, Per Mertesacker, Ver...   \n",
       "2  LA DIGNIDAD COMIENZA POR RESPETAR LA VIDA Y NO...   \n",
       "3                                    @munqueik roba3   \n",
       "4                                     Necesito ayuda   \n",
       "\n",
       "                                           textLemma  \n",
       "0                          modelo huawei don gustavo  \n",
       "1  sol campbell  per mertesacker  vermaelar chapa...  \n",
       "2  dignidad comenzar respetar vida asesinato sele...  \n",
       "3                                              roba3  \n",
       "4                                    necesitar ayuda  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QA9w6dbwfzz"
   },
   "source": [
    "# BERT Fine Tuning for Multi Class Text Classification \n",
    "\n",
    "This notebook contains code to fine tune a pretrained BERT language model to a specific classification task. \n",
    "As BERT model interface the Huggingface library with a PyTorch backend is used.\n",
    "\n",
    "In this notebook, the model has been fine tuned with the English Dataset but could easily be fine tuned on any other classification dataset.\n",
    "\n",
    "The code was implemented based on the huggingface example scripts for glue tasks fine tuning (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128) and the blog post by Chris McCormick (http://mccormickml.com/2019/07/22/BERT-fine-tuning/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tKjjVxVDKsS1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "#from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhzCltg3tR-q",
    "outputId": "5eec36b4-a3b1-4aef-9dd1-c4b84522dfbc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Empty cache of GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZxf_D2gAdTO"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "yJzGKGLX9AxN",
    "outputId": "df5621bd-2a0b-423c-fa55-beca117bad8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=data.copy()\n",
    "df.drop(df[df['textLemma'] == ''].index)\n",
    "\n",
    "### To separate hasgtags word, remove punctuation and low\n",
    "df['Hashtags'] =df.Hashtags.apply(lambda x:Separate_hashtag_words(x)) #delete punctuation\n",
    "df['Hashtags'] = df['Hashtags'].apply(lambda x:remove_punctuation(x))\n",
    "df['Hashtags']= df['Hashtags'].apply(lambda x: x.lower())\n",
    "df['users'] = df['users'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "df['text'] = df.users+' '+df.textLemma+' '+df.Hashtags\n",
    "df[\"text\"] = df[\"text\"].str.replace(r'  ', ' ').str.strip()  # to delete all '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         Vive USA El Universal Mx jajajajajar oir zopi ...\n",
       "age_range                                                    4\n",
       "users                                Vive USA  El Universal Mx\n",
       "Hashtags                       zopilota   la bruja del palacio\n",
       "copy         @Vive_USA @El_Universal_Mx Jajajajaja oye Zopi...\n",
       "textLemma                     jajajajajar oir zopi  ahi hablar\n",
       "Name: 18848, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[18848]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UuN3olL1s1R0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = df.text.values\n",
    "label_cats = df.age_range.astype('category').cat\n",
    "\n",
    "# List of label names (str)\n",
    "label_names = label_cats.categories\n",
    "\n",
    "# List of label ids (int, in range (0,num_classes-1))\n",
    "labels = label_cats.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oracion milagro amen'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSsp_l_yoNJf"
   },
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5UUTkMV4yDTY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "MAX_INPUT_LENGTH = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4-OE6oDZoMIG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT tokenizer.\n",
    "print(f\"Loading {model_name} tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XDkMb1urrIAo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,            \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = MAX_INPUT_LENGTH,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt')\n",
    "  \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KBM6SwDHvirL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  gusgomez1701 YaKoDj TropiBogota Apple HuaweiMobileCo modelo huawei don gustavo\n",
      "Token IDs: tensor([  101, 12670,  3995,  4168,  2480, 16576, 24096,  8038,  3683,  2094,\n",
      "         3501, 19817,  7361, 12322, 22844,  2696,  6207, 23064, 19845, 17751,\n",
      "         3597,  2944,  2080, 23064, 19845,  2123, 24801,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Attention Mask:', attention_masks[0]) # 1 for all text tokens, 0 for all padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vO3MaaVkuI-c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500 training samples\n",
      "3150 validation samples\n",
      "7350 test samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-10-10 train-validation-test split\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.65 * len(dataset))\n",
    "val_size = int(0.105 * len(dataset)) \n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"{train_size} training samples\")\n",
    "print(f\"{val_size} validation samples\")\n",
    "print(f\"{test_size} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZiHoD0KoNpZq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# For test the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1IY9cwiAUCZ"
   },
   "source": [
    "## Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fWfvlMlBg-3_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased model...\n",
      "config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"return_dict\": false,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model for sequence classification\n",
    "print(f\"Loading {model_name} model...\")\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_names)\n",
    "config.output_attentions = True\n",
    "config.return_dict=False\n",
    "print(\"config\", config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WIBG_g36utJA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (6, 768)\n",
      "classifier.bias                                                 (6,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Xzojbxacu1ee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFeBj0EhyGlx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bs5ImW58hZ4-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CbwgopNUhm-M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ub1-0NFChoc6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,219.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  1,219.    Elapsed: 0:00:31.\n",
      "  Batch   120  of  1,219.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  1,219.    Elapsed: 0:01:02.\n",
      "  Batch   200  of  1,219.    Elapsed: 0:01:19.\n",
      "  Batch   240  of  1,219.    Elapsed: 0:01:35.\n",
      "  Batch   280  of  1,219.    Elapsed: 0:01:50.\n",
      "  Batch   320  of  1,219.    Elapsed: 0:02:06.\n",
      "  Batch   360  of  1,219.    Elapsed: 0:02:22.\n",
      "  Batch   400  of  1,219.    Elapsed: 0:02:38.\n",
      "  Batch   440  of  1,219.    Elapsed: 0:02:54.\n",
      "  Batch   480  of  1,219.    Elapsed: 0:03:10.\n",
      "  Batch   520  of  1,219.    Elapsed: 0:03:27.\n",
      "  Batch   560  of  1,219.    Elapsed: 0:03:43.\n",
      "  Batch   600  of  1,219.    Elapsed: 0:04:00.\n",
      "  Batch   640  of  1,219.    Elapsed: 0:04:17.\n",
      "  Batch   680  of  1,219.    Elapsed: 0:04:34.\n",
      "  Batch   720  of  1,219.    Elapsed: 0:04:52.\n",
      "  Batch   760  of  1,219.    Elapsed: 0:05:11.\n",
      "  Batch   800  of  1,219.    Elapsed: 0:05:29.\n",
      "  Batch   840  of  1,219.    Elapsed: 0:05:46.\n",
      "  Batch   880  of  1,219.    Elapsed: 0:06:03.\n",
      "  Batch   920  of  1,219.    Elapsed: 0:06:21.\n",
      "  Batch   960  of  1,219.    Elapsed: 0:06:39.\n",
      "  Batch 1,000  of  1,219.    Elapsed: 0:06:56.\n",
      "  Batch 1,040  of  1,219.    Elapsed: 0:07:15.\n",
      "  Batch 1,080  of  1,219.    Elapsed: 0:07:33.\n",
      "  Batch 1,120  of  1,219.    Elapsed: 0:07:50.\n",
      "  Batch 1,160  of  1,219.    Elapsed: 0:08:08.\n",
      "  Batch 1,200  of  1,219.    Elapsed: 0:08:25.\n",
      "\n",
      "  Average training loss: 1.55\n",
      "  Training epoch took: 0:08:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.38\n",
      "  Validation Loss: 1.54\n",
      "  Validation took: 0:00:27\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,219.    Elapsed: 0:00:17.\n",
      "  Batch    80  of  1,219.    Elapsed: 0:00:35.\n",
      "  Batch   120  of  1,219.    Elapsed: 0:00:53.\n",
      "  Batch   160  of  1,219.    Elapsed: 0:01:10.\n",
      "  Batch   200  of  1,219.    Elapsed: 0:01:28.\n",
      "  Batch   240  of  1,219.    Elapsed: 0:01:47.\n",
      "  Batch   280  of  1,219.    Elapsed: 0:02:05.\n",
      "  Batch   320  of  1,219.    Elapsed: 0:02:24.\n",
      "  Batch   360  of  1,219.    Elapsed: 0:02:41.\n",
      "  Batch   400  of  1,219.    Elapsed: 0:03:01.\n",
      "  Batch   440  of  1,219.    Elapsed: 0:03:19.\n",
      "  Batch   480  of  1,219.    Elapsed: 0:03:37.\n",
      "  Batch   520  of  1,219.    Elapsed: 0:03:54.\n",
      "  Batch   560  of  1,219.    Elapsed: 0:04:12.\n",
      "  Batch   600  of  1,219.    Elapsed: 0:04:30.\n",
      "  Batch   640  of  1,219.    Elapsed: 0:04:47.\n",
      "  Batch   680  of  1,219.    Elapsed: 0:05:05.\n",
      "  Batch   720  of  1,219.    Elapsed: 0:05:23.\n",
      "  Batch   760  of  1,219.    Elapsed: 0:05:42.\n",
      "  Batch   800  of  1,219.    Elapsed: 0:06:00.\n",
      "  Batch   840  of  1,219.    Elapsed: 0:06:17.\n",
      "  Batch   880  of  1,219.    Elapsed: 0:06:34.\n",
      "  Batch   920  of  1,219.    Elapsed: 0:06:52.\n",
      "  Batch   960  of  1,219.    Elapsed: 0:07:09.\n",
      "  Batch 1,000  of  1,219.    Elapsed: 0:07:28.\n",
      "  Batch 1,040  of  1,219.    Elapsed: 0:07:46.\n",
      "  Batch 1,080  of  1,219.    Elapsed: 0:08:04.\n",
      "  Batch 1,120  of  1,219.    Elapsed: 0:08:22.\n",
      "  Batch 1,160  of  1,219.    Elapsed: 0:08:40.\n",
      "  Batch 1,200  of  1,219.    Elapsed: 0:08:57.\n",
      "\n",
      "  Average training loss: 1.42\n",
      "  Training epoch took: 0:09:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40\n",
      "  Validation Loss: 1.49\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,219.    Elapsed: 0:00:18.\n",
      "  Batch    80  of  1,219.    Elapsed: 0:00:36.\n",
      "  Batch   120  of  1,219.    Elapsed: 0:00:54.\n",
      "  Batch   160  of  1,219.    Elapsed: 0:01:12.\n",
      "  Batch   200  of  1,219.    Elapsed: 0:01:31.\n",
      "  Batch   240  of  1,219.    Elapsed: 0:01:49.\n",
      "  Batch   280  of  1,219.    Elapsed: 0:02:06.\n",
      "  Batch   320  of  1,219.    Elapsed: 0:02:24.\n",
      "  Batch   360  of  1,219.    Elapsed: 0:02:42.\n",
      "  Batch   400  of  1,219.    Elapsed: 0:03:00.\n",
      "  Batch   440  of  1,219.    Elapsed: 0:03:19.\n",
      "  Batch   480  of  1,219.    Elapsed: 0:03:39.\n",
      "  Batch   520  of  1,219.    Elapsed: 0:03:58.\n",
      "  Batch   560  of  1,219.    Elapsed: 0:04:16.\n",
      "  Batch   600  of  1,219.    Elapsed: 0:04:34.\n",
      "  Batch   640  of  1,219.    Elapsed: 0:04:52.\n",
      "  Batch   680  of  1,219.    Elapsed: 0:05:10.\n",
      "  Batch   720  of  1,219.    Elapsed: 0:05:27.\n",
      "  Batch   760  of  1,219.    Elapsed: 0:05:45.\n",
      "  Batch   800  of  1,219.    Elapsed: 0:06:02.\n",
      "  Batch   840  of  1,219.    Elapsed: 0:06:20.\n",
      "  Batch   880  of  1,219.    Elapsed: 0:06:37.\n",
      "  Batch   920  of  1,219.    Elapsed: 0:06:55.\n",
      "  Batch   960  of  1,219.    Elapsed: 0:07:13.\n",
      "  Batch 1,000  of  1,219.    Elapsed: 0:07:30.\n",
      "  Batch 1,040  of  1,219.    Elapsed: 0:07:48.\n",
      "  Batch 1,080  of  1,219.    Elapsed: 0:08:05.\n",
      "  Batch 1,120  of  1,219.    Elapsed: 0:08:23.\n",
      "  Batch 1,160  of  1,219.    Elapsed: 0:08:40.\n",
      "  Batch 1,200  of  1,219.    Elapsed: 0:08:58.\n",
      "\n",
      "  Average training loss: 1.30\n",
      "  Training epoch took: 0:09:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.40\n",
      "  Validation Loss: 1.52\n",
      "  Validation took: 0:00:28\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:28:09 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "\n",
    "        loss, logits, attentions = model(input_ids=b_input_ids, \n",
    "                                         attention_mask=b_input_mask, \n",
    "                                         labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits, attentions) = model(input_ids=b_input_ids, \n",
    "                                               attention_mask=b_input_mask,\n",
    "                                               labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Qxik8IiPhvXC",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0:08:33</td>\n",
       "      <td>0:00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.42</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0:09:06</td>\n",
       "      <td>0:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.30</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0:09:07</td>\n",
       "      <td>0:00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               1.55         1.54           0.38       0:08:33         0:00:27\n",
       "2               1.42         1.49           0.40       0:09:06         0:00:29\n",
       "3               1.30         1.52           0.40       0:09:07         0:00:28"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "v2G-EegN85iC",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWmaN_QFZKvd"
   },
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rhhTNEe2ZQM-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels ...\n",
      "Predicted 7350 samples\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print(\"Predicting labels ...\")\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(input_ids=b_input_ids,\n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.extend(logits)\n",
    "  true_labels.extend(label_ids)\n",
    "\n",
    "print(f\"Predicted {len(predictions)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "htdXUiYk7fyB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.40435374149659864\n",
      "Test set Matthews correlation coefficient: 0.22773228601040327\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Test set accuracy: {accuracy_score(true_labels, predictions)}\")\n",
    "print(f\"Test set Matthews correlation coefficient: {matthews_corrcoef(true_labels, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7TMxlhQYB1Bb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set confusiomat: \n",
      "[[  26  307   33   93   11    6]\n",
      " [  18 1064  207  374   25    6]\n",
      " [   8  386  152  608   29    4]\n",
      " [   2  222  121 1607  106    4]\n",
      " [   3  107   96  931  118    5]\n",
      " [   1   82   59  477   47    5]]\n",
      "Test set f1score: 0.2494727095286542\n",
      "Test set accuracy: 0.40435374149659864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Test set confusiomat: \\n{confusion_matrix(true_labels, predictions)}\")\n",
    "print(f\"Test set f1score: {f1_score(true_labels, predictions,average='macro')}\")\n",
    "print(f\"Test set accuracy: {accuracy_score(true_labels, predictions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5EStuaZBGLk"
   },
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "aZIqwQvuBKFZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./bert-base-uncased/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-base-uncased/tokenizer_config.json',\n",
       " './bert-base-uncased/special_tokens_map.json',\n",
       " './bert-base-uncased/vocab.txt',\n",
       " './bert-base-uncased/added_tokens.json',\n",
       " './bert-base-uncased/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = f\"./{model_name}/\"\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "WaXz2CrQhVyo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bert-base-uncased/\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "./bert-base-uncased/ TranferBERT/\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Copy the model files to a directory\n",
    "target_dir = f\"TranferBERT/\"\n",
    "print(output_dir)\n",
    "!echo  $output_dir $target_dir\n",
    "!cp -r $output_dir $target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tXFWVdxh7Zd"
   },
   "source": [
    "## Load Model from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "94-4pMG-h_G_",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co/' to load this model and it looks like TranferBERT/bert-base-uncased/ is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36m_get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    601\u001B[0m                 \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 602\u001B[0;31m                 \u001B[0muser_agent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muser_agent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    603\u001B[0m             )\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   1928\u001B[0m             \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1929\u001B[0;31m             \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1930\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   2124\u001B[0m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_redirects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0metag_timeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2125\u001B[0;31m             \u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2126\u001B[0m             \u001B[0metag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"X-Linked-Etag\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ETag\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(request)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2052\u001B[0;31m     \u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2053\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/requests/models.py\u001B[0m in \u001B[0;36mraise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    942\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhttp_error_msg\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 943\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_error_msg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    944\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: https://huggingface.co/TranferBERT/bert-base-uncased//resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_260528/560124662.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Load a trained model and vocabulary that you have fine-tuned\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoModelForSequenceClassification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    423\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    424\u001B[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001B[0;32m--> 425\u001B[0;31m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_unused_kwargs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    426\u001B[0m             )\n\u001B[1;32m    427\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"auto_map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_map\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    635\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"name_or_path\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0mtrust_remote_code\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"trust_remote_code\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 637\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    638\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"auto_map\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"AutoConfig\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"auto_map\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    639\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    544\u001B[0m         \u001B[0moriginal_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    545\u001B[0m         \u001B[0;31m# Get config dict associated with the base config file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 546\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    547\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    548\u001B[0m         \u001B[0;31m# That config file may point us toward another config file to use.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/whalejaguar/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36m_get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    622\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    623\u001B[0m             raise EnvironmentError(\n\u001B[0;32m--> 624\u001B[0;31m                 \u001B[0;34m\"We couldn't connect to 'https://huggingface.co/' to load this model and it looks like \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    625\u001B[0m                 \u001B[0;34mf\"{pretrained_model_name_or_path} is not the path to a directory conaining a {configuration_file} \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    626\u001B[0m                 \u001B[0;34m\"file.\\nCheckout your internet connection or see how to run the library in offline mode at \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like TranferBERT/bert-base-uncased/ is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = f\"TranferBERT/{model_name}/\"\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT525N9VbR8n"
   },
   "source": [
    "## Predict on some sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI_uBpaWxZaY"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esq849IYbVj2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = [\"ola q hace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2PSbpD9bcqa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "                          text,            \n",
    "                          add_special_tokens = True,\n",
    "                          max_length = 64,\n",
    "                          pad_to_max_length = True,\n",
    "                          return_attention_mask = True,\n",
    "                          return_tensors = 'pt')\n",
    "  \n",
    "  # Add the encoded sentence to the list.    \n",
    "  input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "  # And its attention mask (simply differentiates padding from non-padding).\n",
    "  attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  # Convert the lists into tensors.\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBv57F46cKST",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "last_layer_attentions = []\n",
    "\n",
    "# Move input ids and attention masks to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "attention_masks = attention_masks.to(device)\n",
    "\n",
    "for i in range(len(input_ids)):\n",
    "\n",
    "  ids = input_ids[i].unsqueeze(0)\n",
    "  masks = attention_masks[i].unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(input_ids=ids,\n",
    "                      attention_mask=masks)\n",
    "\n",
    "  # Get logits and compute softmax\n",
    "  logits = outputs[0]\n",
    "  logits = torch.softmax(logits,dim=1)\n",
    "  last_layer_attention = outputs[1][-1]\n",
    "  \n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  last_layer_attention = last_layer_attention.detach().cpu().numpy()\n",
    "\n",
    "  last_layer_attentions.append(last_layer_attention)\n",
    "  predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8t_Ygply0X3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = predictions[0][0]\n",
    "print(\"text:\", texts[0])\n",
    "print(\"predictions:\", probs)\n",
    "pred_idx = np.argmax(probs)\n",
    "print(f\"Prediction: {label_names[pred_idx]} ({probs[pred_idx]:.2f})\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi9mhLaryMNS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(tight_layout=True)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "y_pos = np.arange(len(label_names))\n",
    "confidences = [probs[i] for i in range(len(label_names))]\n",
    "\n",
    "ax.barh(y_pos, confidences, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(label_names)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Confidence')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofcRyMHTxh_f"
   },
   "source": [
    "### Visualize Attentions\n",
    "\n",
    "For each token, visualize the average over all 12 heads of the last layer's attention to the special character [CLS]. The darker the background of the token, the higher its attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3r6jQpFKWWb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lla = last_layer_attentions[0][0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDiU0fPDTgkF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def avg_token_attentions(last_layer_attentions):\n",
    "  return last_layer_attentions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iYtEu5vhJPM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_token_atts = avg_token_attentions(lla)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mpp6Q3EHoovo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def handle_special_token_attentions(tokens, avg_token_atts):\n",
    "  new_tokens = []\n",
    "  new_avg_token_atts = []\n",
    "  for i in range(len(tokens)):\n",
    "    if tokens[i].startswith(\"[\") or tokens[i].startswith(\"##\"):\n",
    "      continue\n",
    "    if i < tokenizer.max_len - 1 and tokens[i+1].startswith(\"##\"):\n",
    "      merged_tokens = tokens[i] + tokens[i+1][2:]\n",
    "      atts = [avg_token_atts[i], avg_token_atts[i+1]]\n",
    "      i += 1\n",
    "      while i < tokenizer.max_len - 1 and tokens[i+1].startswith(\"##\"):\n",
    "        merged_tokens += tokens[i+1][2:]\n",
    "        atts.append(avg_token_atts[i+1])\n",
    "        i += 1\n",
    "      new_tokens.append(merged_tokens)\n",
    "      new_avg_token_atts.append(sum(atts)/len(atts))\n",
    "    elif i < tokenizer.max_len - 1:\n",
    "      new_tokens.append(tokens[i])\n",
    "      new_avg_token_atts.append(avg_token_atts[i])\n",
    "  new_avg_token_atts = new_avg_token_atts / sum(new_avg_token_atts)\n",
    "  return new_tokens, new_avg_token_atts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrCl0RbXmu2W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token, att in zip(tokens, avg_token_atts):\n",
    "  print(token, att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa9tlNnqvgm5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale_color_h_hex(c_h, scale):\n",
    "    return matplotlib.colors.to_hex(\n",
    "        matplotlib.colors.hsv_to_rgb((c_h, scale, 1)))\n",
    "\n",
    "def blue_background_hex(scale):\n",
    "    return scale_color_h_hex(0.625, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "js7LWFLMxoeA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython import display ,html\n",
    "att_html = \"<table><tr>\"\n",
    "for token, att in zip(tokens, avg_token_atts):\n",
    "  att_html += \"<td>\"\n",
    "  att_html += \"<span style=\\\"background-color: \" + blue_background_hex(att) + \"\\\">\" + token + \"</span>\"\n",
    "  att_html += \"</td>\"\n",
    "att_html += \"</tr>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybnrhz1mzfw1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.HTML(att_html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de Copia de BERT_CLF_10kGNAD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "whalejaguar",
   "language": "python",
   "name": "whalejaguar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}